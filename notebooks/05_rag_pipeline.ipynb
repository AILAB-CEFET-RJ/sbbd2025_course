{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval-Augmented Generation with ChromaDB\n",
        "\n",
        "## Learning Goals\n",
        "- Understand the architecture of **RAG** (indexing → retrieval → generation).\n",
        "- Implement a minimal RAG pipeline with **LangChain** and **ChromaDB**.\n",
        "- Compare different chunking strategies and their effect on retrieval.\n",
        "- Query the vector database and inject retrieved context into the LLM prompt.\n",
        "\n",
        "This notebook corresponds to Section *1.6 Retrieval-Augmented Generation (RAG)* in the lecture notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load get_llm.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "# Load environment variables from .env\n",
        "load_dotenv()\n",
        "\n",
        "def get_llm(provider: str = \"openai\"):\n",
        "    \"\"\"\n",
        "    Return a language model instance configured for either OpenAI or Ollama.\n",
        "\n",
        "    This function centralizes the initialization of chat-based LLMs so that \n",
        "    notebooks and applications can switch seamlessly between cloud-based models \n",
        "    (OpenAI) and local models (Ollama).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    provider : str, optional\n",
        "        The backend provider to use. Options are:\n",
        "        - \"openai\": returns a ChatOpenAI instance (requires OPENAI_API_KEY in .env).\n",
        "        - \"ollama\": returns a ChatOllama instance (requires Ollama installed locally).\n",
        "        Default is \"openai\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    langchain.chat_models.base.BaseChatModel\n",
        "        A chat model instance that can be invoked with messages.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    Initialize an OpenAI model (requires API key):\n",
        "\n",
        "    >>> llm = get_llm(\"openai\")\n",
        "    >>> llm.invoke(\"Hello, how are you?\")\n",
        "\n",
        "    Initialize a local Ollama model (e.g., Gemma2 2B):\n",
        "\n",
        "    >>> llm = get_llm(\"ollama\")\n",
        "    >>> llm.invoke(\"Summarize the benefits of reinforcement learning.\")\n",
        "    \"\"\"\n",
        "    if provider == \"openai\":\n",
        "        return ChatOpenAI(\n",
        "            model=\"gpt-4o-mini\",  # can also be \"gpt-4.1\" or \"gpt-4o\"\n",
        "            temperature=0\n",
        "        )\n",
        "    elif provider == \"ollama\":\n",
        "        return ChatOllama(\n",
        "            model=\"gemma2:2b\",   # replace with any local model installed in Ollama\n",
        "            temperature=0\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported provider. Use 'openai' or 'ollama'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "llm = get_llm(\"openai\")  # or get_llm(\"ollama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 — Load sample documents\n",
        "A small text file is created and used for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded document:\n",
            "\n",
            "Databases are organized collections of structured information or data,\n",
            "typically stored electronically in a computer system.\n",
            "Artificial intelligence (AI) refers to systems that can perform tasks that\n",
            "normally require human intelligence, such as reasoning and learning.\n",
            "LLM-based agents combine large language models with external tools.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"\"\"\n",
        "Databases are organized collections of structured information or data,\n",
        "typically stored electronically in a computer system.\n",
        "Artificial intelligence (AI) refers to systems that can perform tasks that\n",
        "normally require human intelligence, such as reasoning and learning.\n",
        "LLM-based agents combine large language models with external tools.\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "with open(\"data/sample_doc.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "loader = TextLoader(\"data/sample_doc.txt\")\n",
        "docs = loader.load()\n",
        "\n",
        "print(\"Loaded document:\")\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 — Chunking\n",
        "The document is split into overlapping chunks to fit into the model context window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks: 5\n",
            "Chunk 0: Databases are organized collections of structured information or data,\n",
            "Chunk 1: typically stored electronically in a computer system.\n",
            "Chunk 2: Artificial intelligence (AI) refers to systems that can perform tasks that\n",
            "Chunk 3: normally require human intelligence, such as reasoning and learning.\n",
            "Chunk 4: LLM-based agents combine large language models with external tools.\n"
          ]
        }
      ],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=80, chunk_overlap=20)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "print(\"Number of chunks:\", len(chunks))\n",
        "for i, c in enumerate(chunks):\n",
        "    print(f\"Chunk {i}: {c.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 — Embedding + Indexing in ChromaDB\n",
        "Each chunk is converted into embeddings and stored in a local ChromaDB instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/91/qbr5td611v3f7_c_dz8zmxmm0000gn/T/ipykernel_40785/3743992889.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a4aab1aff0b47cfb428975bde22a562",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7b14b862bb147c48a3f02845ab44fa3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1afc8158f384a4a922c15711969184d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bfef6fbbcd849199cf03a7b9056a5a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24d13db9db704d76b478685f6b42523b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc2db07fc2b2405aae3488c258445f32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b65f3e4a7c145e59a37a33273f68109",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7b1262600a7467093b5c274eb102080",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "960620ad5403489cb1db8e6bc0cf21e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af3ceac2b6c5480b84a6d9e4e06d2815",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a97c9cee66f6423fb418ba7f29da98a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# embedding_model = HuggingFaceEmbeddings(\n",
        "#   model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "#)\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": \"cpu\"}\n",
        ")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=\"data/chroma_store\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 — Retrieval\n",
        "Semantic search is performed over the vectorstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved chunks:\n",
            "- Artificial intelligence (AI) refers to systems that can perform tasks that\n",
            "- Artificial intelligence (AI) refers to systems that can perform tasks that\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/91/qbr5td611v3f7_c_dz8zmxmm0000gn/T/ipykernel_40785/1810345979.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  results = retriever.get_relevant_documents(query)\n"
          ]
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
        "\n",
        "query = \"What is artificial intelligence?\"\n",
        "results = retriever.get_relevant_documents(query)\n",
        "\n",
        "print(\"Retrieved chunks:\")\n",
        "for r in results:\n",
        "    print(\"-\", r.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 — Generation with retrieved context\n",
        "The retrieved chunks are injected into a prompt template before calling the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A database is an organized collection of structured information or data. It allows for the efficient storage, retrieval, and management of data, often using a database management system (DBMS). Databases can store various types of information and are commonly used in applications ranging from business operations to personal data management. They enable users to perform operations such as querying, updating, and analyzing data in a systematic way.\n",
            "LLM-based agents are systems that integrate large language models (LLMs) with external tools to perform various tasks. These agents leverage the natural language processing capabilities of LLMs to understand and generate human-like text while also utilizing external resources or tools to enhance their functionality. This combination allows them to perform complex tasks, such as answering questions, providing recommendations, or executing commands, by accessing real-time data or specialized applications beyond their training data.\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Use the following context to answer:\\n\\n{context}\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "def rag_pipeline(question: str):\n",
        "    # Retrieve\n",
        "    retrieved_docs = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Generate\n",
        "    chain = prompt_template | llm\n",
        "    return chain.invoke({\"context\": context, \"question\": question}).content\n",
        "\n",
        "print(rag_pipeline(\"What is a database?\"))\n",
        "print(rag_pipeline(\"What are LLM-based agents?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- RAG decouples **knowledge storage** (ChromaDB) from **reasoning** (LLM).\n",
        "- Chunking is critical: too small → fragmented context; too large → exceeds token limits.\n",
        "- The vector database enables **semantic search**, not keyword search.\n",
        "- This architecture is the basis for practical applications like Q&A over documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "1. Replace the sample document with a **PDF loader** (e.g., `PyPDFLoader`) and index a real article.\n",
        "2. Change the chunk size and observe how retrieval quality changes.\n",
        "3. Experiment with different embedding models (`all-MiniLM`, `multi-qa-mpnet-base-dot-v1`).\n",
        "4. Persist the ChromaDB index and reload it in a new notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 embeddings generated.\n",
            "Dimension of each embedding: 1536\n",
            "\n",
            "Similarity matrix:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>The cat sits outside.</th>\n",
              "      <th>It is sunny today.</th>\n",
              "      <th>The dog barks loudly.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>The cat sits outside.</th>\n",
              "      <td>1.00</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>It is sunny today.</th>\n",
              "      <td>0.83</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The dog barks loudly.</th>\n",
              "      <td>0.83</td>\n",
              "      <td>0.79</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       The cat sits outside.  It is sunny today.  \\\n",
              "The cat sits outside.                   1.00                0.83   \n",
              "It is sunny today.                      0.83                1.00   \n",
              "The dog barks loudly.                   0.83                0.79   \n",
              "\n",
              "                       The dog barks loudly.  \n",
              "The cat sits outside.                   0.83  \n",
              "It is sunny today.                      0.79  \n",
              "The dog barks loudly.                   1.00  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Example texts (chunks from a document)\n",
        "texts = [\n",
        "    \"The cat sits outside.\",\n",
        "    \"It is sunny today.\",\n",
        "    \"The dog barks loudly.\"\n",
        "]\n",
        "\n",
        "# Create embedding model\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "# Generate vector representations\n",
        "vectors = embedding_model.embed_documents(texts)\n",
        "\n",
        "print(len(vectors), \"embeddings generated.\")\n",
        "print(\"Dimension of each embedding:\", len(vectors[0]))\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "# Pretty print with pandas DataFrame\n",
        "df = pd.DataFrame(similarity_matrix, index=texts, columns=texts)\n",
        "print(\"\\nSimilarity matrix:\")\n",
        "df.round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chuncks:\n",
            "['The cat sits', 'sits outside. It is', 'It is sunny today.', 'The dog barks', 'barks loudly.']\n",
            "Query: What is the weather like?\n",
            "Result:\n",
            "It is sunny today.\n",
            "sits outside. It is\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Example document\n",
        "text = \"The cat sits outside. It is sunny today. The dog barks loudly.\"\n",
        "chunks = CharacterTextSplitter(chunk_size=20, chunk_overlap=5, separator=\" \").split_text(text)\n",
        "\n",
        "print(\"Chuncks:\")\n",
        "print(chunks)\n",
        "\n",
        "# Embedding model\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "# Store chunks + embeddings in Chroma vector DB\n",
        "vectorstore = Chroma.from_texts(chunks, embedding_model)\n",
        "\n",
        "# Example query\n",
        "query = \"What is the weather like?\"\n",
        "docs = vectorstore.similarity_search(query, k=2)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Result:\")\n",
        "for d in docs:\n",
        "    print(d.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['The cat sits', 'sits outside. It is', 'It is sunny today.', 'The dog barks', 'barks loudly.']\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text = \"The cat sits outside. It is sunny today. The dog barks loudly.\"\n",
        "splitter = CharacterTextSplitter(chunk_size=20, chunk_overlap=5, separator=\" \")\n",
        "chunks = splitter.split_text(text)\n",
        "\n",
        "print(chunks)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sbbd2025_course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
