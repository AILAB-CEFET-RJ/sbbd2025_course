{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval-Augmented Generation with ChromaDB\n",
        "\n",
        "## Learning Goals\n",
        "- Understand the architecture of **RAG** (indexing → retrieval → generation).\n",
        "- Implement a minimal RAG pipeline with **LangChain** and **ChromaDB**.\n",
        "- Compare different chunking strategies and their effect on retrieval.\n",
        "- Query the vector database and inject retrieved context into the LLM prompt.\n",
        "\n",
        "This notebook corresponds to Section *1.6 Retrieval-Augmented Generation (RAG)* in the lecture notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load get_llm.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "# Load environment variables from .env\n",
        "load_dotenv()\n",
        "\n",
        "def get_llm(provider: str = \"openai\"):\n",
        "    \"\"\"\n",
        "    Return a language model instance configured for either OpenAI or Ollama.\n",
        "\n",
        "    This function centralizes the initialization of chat-based LLMs so that \n",
        "    notebooks and applications can switch seamlessly between cloud-based models \n",
        "    (OpenAI) and local models (Ollama).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    provider : str, optional\n",
        "        The backend provider to use. Options are:\n",
        "        - \"openai\": returns a ChatOpenAI instance (requires OPENAI_API_KEY in .env).\n",
        "        - \"ollama\": returns a ChatOllama instance (requires Ollama installed locally).\n",
        "        Default is \"openai\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    langchain.chat_models.base.BaseChatModel\n",
        "        A chat model instance that can be invoked with messages.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    Initialize an OpenAI model (requires API key):\n",
        "\n",
        "    >>> llm = get_llm(\"openai\")\n",
        "    >>> llm.invoke(\"Hello, how are you?\")\n",
        "\n",
        "    Initialize a local Ollama model (e.g., Gemma2 2B):\n",
        "\n",
        "    >>> llm = get_llm(\"ollama\")\n",
        "    >>> llm.invoke(\"Summarize the benefits of reinforcement learning.\")\n",
        "    \"\"\"\n",
        "    if provider == \"openai\":\n",
        "        return ChatOpenAI(\n",
        "            model=\"gpt-4o-mini\",  # can also be \"gpt-4.1\" or \"gpt-4o\"\n",
        "            temperature=0\n",
        "        )\n",
        "    elif provider == \"ollama\":\n",
        "        return ChatOllama(\n",
        "            model=\"gemma2:2b\",   # replace with any local model installed in Ollama\n",
        "            temperature=0\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported provider. Use 'openai' or 'ollama'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "llm = get_llm(\"openai\")  # or get_llm(\"ollama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 — Load sample documents\n",
        "A small text file is created and used for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded document:\n",
            "\n",
            "Databases are organized collections of structured information or data, typically stored electronically in a computer system. Artificial intelligence (AI) refers to systems that can perform tasks that normally require human intelligence, such as reasoning and learning. LLM-based agents combine large language models with external tools.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"\"\"\n",
        "Databases are organized collections of structured information or data, typically stored electronically in a computer system. Artificial intelligence (AI) refers to systems that can perform tasks that normally require human intelligence, such as reasoning and learning. LLM-based agents combine large language models with external tools.\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "with open(\"data/sample_doc.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "loader = TextLoader(\"data/sample_doc.txt\")\n",
        "docs = loader.load()\n",
        "\n",
        "print(\"Loaded document:\")\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 — Chunking\n",
        "The document is split into overlapping chunks to fit into the model context window.\n",
        "Concretely, this code splits long documents into smaller overlapping text chunks.\n",
        "This is a preprocessing step commonly used in LLM agents, especially in:\n",
        "\n",
        "- Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "- Tool-using agents with document memory\n",
        "\n",
        "- Long-context reasoning and planning\n",
        "\n",
        "- LLMs work best when they receive short, focused pieces of text, rather than very long documents.\n",
        "\n",
        "`RecursiveCharacterTextSplitter` does not blindly cut text every chunk_size characters. Instead, it tries to split text hierarchically, using separators like:\n",
        "\n",
        "- Paragraph breaks (\\n\\n)\n",
        "\n",
        "- Line breaks (\\n)\n",
        "\n",
        "- Sentences\n",
        "\n",
        "- Words\n",
        "\n",
        "- Characters (last resort)\n",
        "\n",
        "Only when a unit is too large does it apply character-level splitting with overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks: 6\n",
            "Chunk 0: Databases are organized collections of structured information or data,\n",
            "Chunk 1: or data, typically stored electronically in a computer system. Artificial\n",
            "Chunk 2: system. Artificial intelligence (AI) refers to systems that can perform tasks\n",
            "Chunk 3: can perform tasks that normally require human intelligence, such as reasoning\n",
            "Chunk 4: such as reasoning and learning. LLM-based agents combine large language models\n",
            "Chunk 5: language models with external tools.\n"
          ]
        }
      ],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=80, chunk_overlap=20)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "print(\"Number of chunks:\", len(chunks))\n",
        "for i, c in enumerate(chunks):\n",
        "    print(f\"Chunk {i}: {c.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 — Embedding + Indexing in ChromaDB\n",
        "Each chunk is converted into embeddings and stored in a local ChromaDB instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# embedding_model = HuggingFaceEmbeddings(\n",
        "#   model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "#)\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": \"cpu\"}\n",
        ")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=\"data/chroma_store\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 — Retrieval\n",
        "Semantic search is performed over the vectorstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved chunks:\n",
            "- system. Artificial intelligence (AI) refers to systems that can perform tasks\n",
            "- system. Artificial intelligence (AI) refers to systems that can perform tasks\n"
          ]
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
        "\n",
        "query = \"What is artificial intelligence?\"\n",
        "results = retriever.get_relevant_documents(query)\n",
        "\n",
        "print(\"Retrieved chunks:\")\n",
        "for r in results:\n",
        "    print(\"-\", r.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 — Generation with retrieved context\n",
        "The retrieved chunks are injected into a prompt template before calling the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant. Use the context provided below to answer. \"\n",
        "        \"If the context does not provide information to answer, say 'I dont know.'.\\n\"\n",
        "        \"BEGIN of CONTEXT\\n{context}\\nEND OF CONTEXT\"\n",
        "    ),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "\n",
        "def rag_pipeline(question: str):\n",
        "    # 1. Retrieve relevant documents\n",
        "    retrieved_docs = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # 2. Generate answer\n",
        "    chain = prompt_template | llm\n",
        "    answer = chain.invoke(\n",
        "        {\"context\": context, \"question\": question}\n",
        "    ).content\n",
        "\n",
        "    # 3. Nicely formatted output\n",
        "    print(\"=\" * 60)\n",
        "    print(\"QUERY:\")\n",
        "    print(question)\n",
        "    print(\"-\" * 60)\n",
        "    print(\"ANSWER:\")\n",
        "    print(answer)\n",
        "    print(\"=\" * 60)\n",
        "    print()  # blank line for readability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "QUERY:\n",
            "What is a database?\n",
            "------------------------------------------------------------\n",
            "ANSWER:\n",
            "A database is an organized collection of structured information or data.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "QUERY:\n",
            "What are LLM-based agents?\n",
            "------------------------------------------------------------\n",
            "ANSWER:\n",
            "LLM-based agents are systems that utilize large language models (LLMs) to perform tasks that involve reasoning and learning. These agents leverage the capabilities of LLMs to understand and generate human-like text, enabling them to engage in various applications such as conversation, information retrieval, and problem-solving.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "QUERY:\n",
            "What is artificial intelligence?\n",
            "------------------------------------------------------------\n",
            "ANSWER:\n",
            "Artificial intelligence (AI) refers to systems that can perform tasks.\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "QUERY:\n",
            "What is Quantum Physics?\n",
            "------------------------------------------------------------\n",
            "ANSWER:\n",
            "I don't know.\n",
            "============================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rag_pipeline(\"What is a database?\")\n",
        "rag_pipeline(\"What are LLM-based agents?\")\n",
        "rag_pipeline(\"What is artificial intelligence?\")\n",
        "rag_pipeline(\"What is Quantum Physics?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- RAG decouples **knowledge storage** (ChromaDB) from **reasoning** (LLM).\n",
        "- Chunking is critical: too small → fragmented context; too large → exceeds token limits.\n",
        "- The vector database enables **semantic search**, not keyword search.\n",
        "- This architecture is the basis for practical applications like Q&A over documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "1. Replace the sample document with a **PDF loader** (e.g., `PyPDFLoader`) and index a real article.\n",
        "2. Change the chunk size and observe how retrieval quality changes.\n",
        "3. Experiment with different embedding models (`all-MiniLM`, `multi-qa-mpnet-base-dot-v1`).\n",
        "4. Persist the ChromaDB index and reload it in a new notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Misc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 embeddings generated.\n",
            "Dimension of each embedding: 1536\n",
            "\n",
            "Similarity matrix:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>The cat sits outside.</th>\n",
              "      <th>It is sunny today.</th>\n",
              "      <th>The dog barks loudly.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>The cat sits outside.</th>\n",
              "      <td>1.00</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>It is sunny today.</th>\n",
              "      <td>0.83</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The dog barks loudly.</th>\n",
              "      <td>0.83</td>\n",
              "      <td>0.79</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       The cat sits outside.  It is sunny today.  \\\n",
              "The cat sits outside.                   1.00                0.83   \n",
              "It is sunny today.                      0.83                1.00   \n",
              "The dog barks loudly.                   0.83                0.79   \n",
              "\n",
              "                       The dog barks loudly.  \n",
              "The cat sits outside.                   0.83  \n",
              "It is sunny today.                      0.79  \n",
              "The dog barks loudly.                   1.00  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Example texts (chunks from a document)\n",
        "texts = [\n",
        "    \"The cat sits outside.\",\n",
        "    \"It is sunny today.\",\n",
        "    \"The dog barks loudly.\"\n",
        "]\n",
        "\n",
        "# Create embedding model\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "# Generate vector representations\n",
        "vectors = embedding_model.embed_documents(texts)\n",
        "\n",
        "print(len(vectors), \"embeddings generated.\")\n",
        "print(\"Dimension of each embedding:\", len(vectors[0]))\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "\n",
        "# Pretty print with pandas DataFrame\n",
        "df = pd.DataFrame(similarity_matrix, index=texts, columns=texts)\n",
        "print(\"\\nSimilarity matrix:\")\n",
        "df.round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chuncks:\n",
            "['The cat sits', 'sits outside. It is', 'It is sunny today.', 'The dog barks', 'barks loudly.']\n",
            "Query: What is the weather like?\n",
            "Result:\n",
            "It is sunny today.\n",
            "It is sunny today.\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Example document\n",
        "text = \"The cat sits outside. It is sunny today. The dog barks loudly.\"\n",
        "chunks = CharacterTextSplitter(chunk_size=20, chunk_overlap=5, separator=\" \").split_text(text)\n",
        "\n",
        "print(\"Chuncks:\")\n",
        "print(chunks)\n",
        "\n",
        "# Embedding model\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "# Store chunks + embeddings in Chroma vector DB\n",
        "vectorstore = Chroma.from_texts(chunks, embedding_model)\n",
        "\n",
        "# Example query\n",
        "query = \"What is the weather like?\"\n",
        "docs = vectorstore.similarity_search(query, k=2)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Result:\")\n",
        "for d in docs:\n",
        "    print(d.page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['The cat sits', 'sits outside. It is', 'It is sunny today.', 'The dog barks', 'barks loudly.']\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text = \"The cat sits outside. It is sunny today. The dog barks loudly.\"\n",
        "splitter = CharacterTextSplitter(chunk_size=20, chunk_overlap=5, separator=\" \")\n",
        "chunks = splitter.split_text(text)\n",
        "\n",
        "print(chunks)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sbbd2025_course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
