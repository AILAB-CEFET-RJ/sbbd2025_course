{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval-Augmented Generation with ChromaDB\n",
        "\n",
        "## Learning Goals\n",
        "- Understand the architecture of **RAG** (indexing → retrieval → generation).\n",
        "- Implement a minimal RAG pipeline with **LangChain** and **ChromaDB**.\n",
        "- Compare different chunking strategies and their effect on retrieval.\n",
        "- Query the vector database and inject retrieved context into the LLM prompt.\n",
        "\n",
        "This notebook corresponds to Section *1.6 Retrieval-Augmented Generation (RAG)* in the lecture notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load get_llm.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "# Load environment variables from .env\n",
        "load_dotenv()\n",
        "\n",
        "def get_llm(provider: str = \"openai\"):\n",
        "    \"\"\"\n",
        "    Return a language model instance configured for either OpenAI or Ollama.\n",
        "\n",
        "    This function centralizes the initialization of chat-based LLMs so that \n",
        "    notebooks and applications can switch seamlessly between cloud-based models \n",
        "    (OpenAI) and local models (Ollama).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    provider : str, optional\n",
        "        The backend provider to use. Options are:\n",
        "        - \"openai\": returns a ChatOpenAI instance (requires OPENAI_API_KEY in .env).\n",
        "        - \"ollama\": returns a ChatOllama instance (requires Ollama installed locally).\n",
        "        Default is \"openai\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    langchain.chat_models.base.BaseChatModel\n",
        "        A chat model instance that can be invoked with messages.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    Initialize an OpenAI model (requires API key):\n",
        "\n",
        "    >>> llm = get_llm(\"openai\")\n",
        "    >>> llm.invoke(\"Hello, how are you?\")\n",
        "\n",
        "    Initialize a local Ollama model (e.g., Gemma2 2B):\n",
        "\n",
        "    >>> llm = get_llm(\"ollama\")\n",
        "    >>> llm.invoke(\"Summarize the benefits of reinforcement learning.\")\n",
        "    \"\"\"\n",
        "    if provider == \"openai\":\n",
        "        return ChatOpenAI(\n",
        "            model=\"gpt-4o-mini\",  # can also be \"gpt-4.1\" or \"gpt-4o\"\n",
        "            temperature=0\n",
        "        )\n",
        "    elif provider == \"ollama\":\n",
        "        return ChatOllama(\n",
        "            model=\"gemma2:2b\",   # replace with any local model installed in Ollama\n",
        "            temperature=0\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported provider. Use 'openai' or 'ollama'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "# !pip install chromadb langchain langchain-community sentence-transformers\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# It is assumed that get_llm() is defined elsewhere (Notebook 2 helper)\n",
        "llm = get_llm(\"openai\")  # or get_llm(\"ollama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 — Load sample documents\n",
        "A small text file is created and used for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded document:\n",
            "\n",
            "Databases are organized collections of structured information or data,\n",
            "typically stored electronically in a computer system.\n",
            "Artificial intelligence (AI) refers to systems that can perform tasks that\n",
            "normally require human intelligence, such as reasoning and learning.\n",
            "LLM-based agents combine large language models with external tools.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"\"\"\n",
        "Databases are organized collections of structured information or data,\n",
        "typically stored electronically in a computer system.\n",
        "Artificial intelligence (AI) refers to systems that can perform tasks that\n",
        "normally require human intelligence, such as reasoning and learning.\n",
        "LLM-based agents combine large language models with external tools.\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "with open(\"data/sample_doc.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "loader = TextLoader(\"data/sample_doc.txt\")\n",
        "docs = loader.load()\n",
        "\n",
        "print(\"Loaded document:\")\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 — Chunking\n",
        "The document is split into overlapping chunks to fit into the model context window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks: 5\n",
            "Chunk 0: Databases are organized collections of structured information or data,\n",
            "Chunk 1: typically stored electronically in a computer system.\n",
            "Chunk 2: Artificial intelligence (AI) refers to systems that can perform tasks that\n",
            "Chunk 3: normally require human intelligence, such as reasoning and learning.\n",
            "Chunk 4: LLM-based agents combine large language models with external tools.\n"
          ]
        }
      ],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=80, chunk_overlap=20)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "print(\"Number of chunks:\", len(chunks))\n",
        "for i, c in enumerate(chunks):\n",
        "    print(f\"Chunk {i}: {c.page_content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 — Embedding + Indexing in ChromaDB\n",
        "Each chunk is converted into embeddings and stored in a local ChromaDB instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": \"cpu\"}\n",
        ")\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=\"data/chroma_store\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 — Retrieval\n",
        "Semantic search is performed over the vectorstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved chunks:\n",
            "- Artificial intelligence (AI) refers to systems that can perform tasks that\n",
            "- Artificial intelligence (AI) refers to systems that can perform tasks that\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1886745/1810345979.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  results = retriever.get_relevant_documents(query)\n"
          ]
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
        "\n",
        "query = \"What is artificial intelligence?\"\n",
        "results = retriever.get_relevant_documents(query)\n",
        "\n",
        "print(\"Retrieved chunks:\")\n",
        "for r in results:\n",
        "    print(\"-\", r.page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 — Generation with retrieved context\n",
        "The retrieved chunks are injected into a prompt template before calling the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A database is an organized collection of structured information or data. It allows for efficient storage, retrieval, and management of data, often using a database management system (DBMS). Databases can store various types of information and are commonly used in applications ranging from business operations to personal data management.\n",
            "LLM-based agents are systems that integrate large language models (LLMs) with external tools and resources to perform various tasks. These agents leverage the natural language processing capabilities of LLMs to understand and generate human-like text, while also utilizing external tools to enhance their functionality. This combination allows them to perform complex operations, such as retrieving information, executing commands, or interacting with other software applications, making them versatile in applications like customer support, content generation, and data analysis.\n"
          ]
        }
      ],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant. Use the following context to answer:\\n\\n{context}\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "def rag_pipeline(question: str):\n",
        "    # Retrieve\n",
        "    retrieved_docs = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "    # Generate\n",
        "    chain = prompt_template | llm\n",
        "    return chain.invoke({\"context\": context, \"question\": question}).content\n",
        "\n",
        "print(rag_pipeline(\"What is a database?\"))\n",
        "print(rag_pipeline(\"What are LLM-based agents?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- RAG decouples **knowledge storage** (ChromaDB) from **reasoning** (LLM).\n",
        "- Chunking is critical: too small → fragmented context; too large → exceeds token limits.\n",
        "- The vector database enables **semantic search**, not keyword search.\n",
        "- This architecture is the basis for practical applications like Q&A over documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "1. Replace the sample document with a **PDF loader** (e.g., `PyPDFLoader`) and index a real article.\n",
        "2. Change the chunk size and observe how retrieval quality changes.\n",
        "3. Experiment with different embedding models (`all-MiniLM`, `multi-qa-mpnet-base-dot-v1`).\n",
        "4. Persist the ChromaDB index and reload it in a new notebook."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (sbbd2025_course)",
      "language": "python",
      "name": "sbbd2025_course"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
