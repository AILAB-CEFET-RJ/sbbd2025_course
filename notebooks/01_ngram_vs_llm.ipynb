{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# From Statistical Models to LLMs\n",
        "## Learning Goals\n",
        "- Understand how statistical n-gram models work.\n",
        "- Compare their limitations with modern LLMs.\n",
        "- See how context window size impacts predictions.\n",
        "- Experience emergent behaviors in transformer-based models.\n",
        "\n",
        "This notebook connects with Section *1.2 From Statistical Language Models to Neural-based LLMs* of the lecture notes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load a sample text corpus\n",
        "We’ll use the **Reuters dataset** from NLTK, which contains short news articles.  \n",
        "This will serve as training data for our n-gram models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /home/ebezerra/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/ebezerra/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/ebezerra/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens: 142731\n",
            "Sample tokens: ['asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's', '.-', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u', '.', 's', '.', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asia', \"'\"]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "sentences = reuters.sents(categories='trade')\n",
        "tokens = [t.lower() for sent in sentences for t in sent]\n",
        "print('Number of tokens:', len(tokens))\n",
        "print('Sample tokens:', tokens[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Build a simple n-gram model\n",
        "- A **bigram** model looks at the last 1 token.\n",
        "- A **trigram** model looks at the last 2 tokens.\n",
        "\n",
        "We’ll count frequencies and use them to predict the next word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram sample: drafting amendments to the u . s . s . s . s . s . s . s . s\n",
            "Trigram sample: last this century and the united states , the central bank said . \" the u . s . trade deficit ,\n"
          ]
        }
      ],
      "source": [
        "def build_ngram_model(tokens, n=2):\n",
        "    model = defaultdict(Counter)\n",
        "    for i in range(len(tokens)-n):\n",
        "        context = tuple(tokens[i:i+n-1])\n",
        "        next_word = tokens[i+n-1]\n",
        "        model[context][next_word] += 1\n",
        "    return model\n",
        "\n",
        "bigram_model = build_ngram_model(tokens, n=2)\n",
        "trigram_model = build_ngram_model(tokens, n=3)\n",
        "\n",
        "def generate_text(model, n=2, length=20, seed=None):\n",
        "    if not seed:\n",
        "        seed = random.choice(list(model.keys()))\n",
        "    output = list(seed)\n",
        "    for _ in range(length):\n",
        "        context = tuple(output[-(n-1):])\n",
        "        if context not in model:\n",
        "            break\n",
        "        next_word = model[context].most_common(1)[0][0]\n",
        "        output.append(next_word)\n",
        "    return ' '.join(output)\n",
        "\n",
        "print('Bigram sample:', generate_text(bigram_model, n=2))\n",
        "print('Trigram sample:', generate_text(trigram_model, n=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- Does the output feel natural or broken?\n",
        "- Notice how **short context** (bigram/trigram) limits coherence.\n",
        "- Try changing the `length` parameter and observe when it becomes gibberish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Use a Transformer-based LLM (GPT-2)\n",
        "Now let’s compare with a pretrained HuggingFace model.  \n",
        "Unlike n-grams, GPT-2 has a context window of **1024 tokens** and was trained on a massive dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The future of artificial intelligence in databases is uncertain. As some of the companies that are working on AI with the help of IBM come up with new ways to manage and process data, such as the DeepMind AI project, it is possible that there will be a shift to a more collaborative approach to AI.\n",
            "\n",
            "Answers to Questions\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline('text-generation', model='gpt2', device=-1)  # device=-1 = CPU\n",
        "\n",
        "prompt = 'The future of artificial intelligence in databases'\n",
        "output = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Generated Text </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────────────────────────────────────</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[92m──────────────────────────────────────────────── \u001b[0m\u001b[1;34m Generated Text \u001b[0m\u001b[92m ─────────────────────────────────────────────────\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The future of artificial intelligence in databases is uncertain. As some of the companies that are working on AI   \n",
              "with the help of IBM come up with new ways to manage and process data, such as the DeepMind AI project, it is      \n",
              "possible that there will be a shift to a more collaborative approach to AI.                                        \n",
              "\n",
              "Answers to Questions                                                                                               \n",
              "</pre>\n"
            ],
            "text/plain": [
              "The future of artificial intelligence in databases is uncertain. As some of the companies that are working on AI   \n",
              "with the help of IBM come up with new ways to manage and process data, such as the DeepMind AI project, it is      \n",
              "possible that there will be a shift to a more collaborative approach to AI.                                        \n",
              "\n",
              "Answers to Questions                                                                                               \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "\n",
        "console = Console()\n",
        "text = output[0]['generated_text']\n",
        "\n",
        "console.rule(\"[bold blue] Generated Text [/bold blue]\")\n",
        "console.print(Markdown(text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- The GPT-2 output is more coherent, even though the model is relatively small.\n",
        "- Unlike n-grams, it can capture longer dependencies.\n",
        "- This illustrates the **scaling → emergent capabilities** phenomenon.\n",
        "\n",
        "## Exercises\n",
        "1. Train a 4-gram model and compare with the bigram/trigram. Does it improve coherence?\n",
        "2. Replace `gpt2` with a larger HuggingFace model (e.g., `distilgpt2`, `gpt2-medium`) and compare.\n",
        "3. Change the `prompt` and observe how the model continues your text."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (sbbd2025_course)",
      "language": "python",
      "name": "sbbd2025_course"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
