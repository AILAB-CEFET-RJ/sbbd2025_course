{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# From Statistical Models to LLMs\n",
        "## Learning Goals\n",
        "- Understand how statistical n-gram models work.\n",
        "- Compare their limitations with modern LLMs.\n",
        "- See how context window size impacts predictions.\n",
        "- Experience emergent behaviors in transformer-based models.\n",
        "\n",
        "This notebook connects with Section *1.2 From Statistical Language Models to Neural-based LLMs* of the lecture notes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load a sample text corpus\n",
        "We’ll use the **Reuters dataset** from NLTK, which contains short news articles.  \n",
        "This will serve as training data for our n-gram models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to\n",
            "[nltk_data]     /Users/ebezerra/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/ebezerra/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/ebezerra/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tokens: 142731\n",
            "Sample tokens: ['asian', 'exporters', 'fear', 'damage', 'from', 'u', '.', 's', '.-', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u', '.', 's', '.', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asia', \"'\"]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "sentences = reuters.sents(categories='trade')\n",
        "tokens = [t.lower() for sent in sentences for t in sent]\n",
        "print('Number of tokens:', len(tokens))\n",
        "print('Sample tokens:', tokens[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Build a simple n-gram model\n",
        "- A **bigram** model looks at the last 1 token.\n",
        "- A **trigram** model looks at the last 2 tokens.\n",
        "\n",
        "We’ll count frequencies and use them to predict the next word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bigram sample: printers , the u . s . s . s . s . s . s . s . s .\n",
            "Trigram sample: switch from export to domestic - led growth , and the united states , the central bank said . \" the u\n"
          ]
        }
      ],
      "source": [
        "def build_ngram_model(tokens, n=2):\n",
        "    model = defaultdict(Counter)\n",
        "    for i in range(len(tokens)-n):\n",
        "        context = tuple(tokens[i:i+n-1])\n",
        "        next_word = tokens[i+n-1]\n",
        "        model[context][next_word] += 1\n",
        "    return model\n",
        "\n",
        "bigram_model = build_ngram_model(tokens, n=2)\n",
        "trigram_model = build_ngram_model(tokens, n=3)\n",
        "\n",
        "def generate_text(model, n=2, length=20, seed=None):\n",
        "    if not seed:\n",
        "        seed = random.choice(list(model.keys()))\n",
        "    output = list(seed)\n",
        "    for _ in range(length):\n",
        "        context = tuple(output[-(n-1):])\n",
        "        if context not in model:\n",
        "            break\n",
        "        next_word = model[context].most_common(1)[0][0]\n",
        "        output.append(next_word)\n",
        "    return ' '.join(output)\n",
        "\n",
        "print('Bigram sample:', generate_text(bigram_model, n=2))\n",
        "print('Trigram sample:', generate_text(trigram_model, n=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- Does the output feel natural or broken?\n",
        "- Notice how **short context** (bigram/trigram) limits coherence.\n",
        "- Try changing the `length` parameter and observe when it becomes gibberish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Use a Transformer-based LLM (GPT-2)\n",
        "Now let’s compare with a pretrained HuggingFace model.  \n",
        "Unlike n-grams, GPT-2 has a context window of **1024 tokens** and was trained on a massive dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The future of artificial intelligence in databases are now an open question, and it is possible that the possibilities for new technologies will grow and may even be explored. However, the fact that we can predict how new technologies will perform and how well they will work in the future is something that is of high importance to the AI community as they need to prepare for the future.\n",
            "\n",
            "The present state of AI is very different from that of the past. As many of you know, AI has been around for more than a century. It is a new form of intelligence that's new and exciting to the world's population and that is what is driving the development of the AI community. The future of AI is also of great concern, and in that sense this is the most important question we are facing.\n",
            "\n",
            "It is not easy to predict when the next big change will be in the world of AI. It is important to remember that AI is a process and that every decision that can be made is governed by a set of rules, but if you look at the evolution of machine learning we are seeing a very different development. It's not that machines are smarter than humans and that they have to perform certain tasks to understand algorithms, but rather that there is a certain level of sophistication and a certain level of sophistication that can\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline('text-generation', model='gpt2', device=-1)  # device=-1 = CPU\n",
        "\n",
        "prompt = 'The future of artificial intelligence in databases'\n",
        "output = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.26.4\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print(np.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Generated Text </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────────────────────────────────────</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[92m──────────────────────────────────────────────── \u001b[0m\u001b[1;34m Generated Text \u001b[0m\u001b[92m ─────────────────────────────────────────────────\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The future of artificial intelligence in databases are now an open question, and it is possible that the           \n",
              "possibilities for new technologies will grow and may even be explored. However, the fact that we can predict how   \n",
              "new technologies will perform and how well they will work in the future is something that is of high importance to \n",
              "the AI community as they need to prepare for the future.                                                           \n",
              "\n",
              "The present state of AI is very different from that of the past. As many of you know, AI has been around for more  \n",
              "than a century. It is a new form of intelligence that's new and exciting to the world's population and that is what\n",
              "is driving the development of the AI community. The future of AI is also of great concern, and in that sense this  \n",
              "is the most important question we are facing.                                                                      \n",
              "\n",
              "It is not easy to predict when the next big change will be in the world of AI. It is important to remember that AI \n",
              "is a process and that every decision that can be made is governed by a set of rules, but if you look at the        \n",
              "evolution of machine learning we are seeing a very different development. It's not that machines are smarter than  \n",
              "humans and that they have to perform certain tasks to understand algorithms, but rather that there is a certain    \n",
              "level of sophistication and a certain level of sophistication that can                                             \n",
              "</pre>\n"
            ],
            "text/plain": [
              "The future of artificial intelligence in databases are now an open question, and it is possible that the           \n",
              "possibilities for new technologies will grow and may even be explored. However, the fact that we can predict how   \n",
              "new technologies will perform and how well they will work in the future is something that is of high importance to \n",
              "the AI community as they need to prepare for the future.                                                           \n",
              "\n",
              "The present state of AI is very different from that of the past. As many of you know, AI has been around for more  \n",
              "than a century. It is a new form of intelligence that's new and exciting to the world's population and that is what\n",
              "is driving the development of the AI community. The future of AI is also of great concern, and in that sense this  \n",
              "is the most important question we are facing.                                                                      \n",
              "\n",
              "It is not easy to predict when the next big change will be in the world of AI. It is important to remember that AI \n",
              "is a process and that every decision that can be made is governed by a set of rules, but if you look at the        \n",
              "evolution of machine learning we are seeing a very different development. It's not that machines are smarter than  \n",
              "humans and that they have to perform certain tasks to understand algorithms, but rather that there is a certain    \n",
              "level of sophistication and a certain level of sophistication that can                                             \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "\n",
        "console = Console()\n",
        "text = output[0]['generated_text']\n",
        "\n",
        "console.rule(\"[bold blue] Generated Text [/bold blue]\")\n",
        "console.print(Markdown(text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- The GPT-2 output is more coherent, even though the model is relatively small.\n",
        "- Unlike n-grams, it can capture longer dependencies.\n",
        "- This illustrates the **scaling → emergent capabilities** phenomenon.\n",
        "\n",
        "## Exercises\n",
        "1. Train a 4-gram model and compare with the bigram/trigram. Does it improve coherence?\n",
        "2. Replace `gpt2` with a larger HuggingFace model (e.g., `distilgpt2`, `gpt2-medium`) and compare.\n",
        "3. Change the `prompt` and observe how the model continues your text."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sbbd2025_course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
