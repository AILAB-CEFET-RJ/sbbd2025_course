{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Tokens with `tiktoken`\n",
        "\n",
        "## Learning Goals\n",
        "- Understand how text is represented as **tokens** for LLMs.\n",
        "- Use the **tiktoken** library to encode and decode tokens.\n",
        "- See how token count affects **context windows** and **costs**.\n",
        "\n",
        "This notebook connects with the discussion of **context windows** in Section 1.2 of the lecture notes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install and import `tiktoken`\n",
        "`tiktoken` is an OpenAI library for **tokenization**. Each model (GPT-3.5, GPT-4, GPT-5, etc.) has a specific encoding scheme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load an encoding\n",
        "- For GPT-3.5 / GPT-4, the common encoding is `cl100k_base`.\n",
        "- For Codex models, it's `p50k_base`.\n",
        "- For older GPT-2, it's `gpt2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: [791, 1989, 315, 17863, 39524, 555, 9641, 469, 13, 13934, 952]\n",
            "Number of tokens: 11\n",
            "Decoded back: The art of Computer Programming by Donald E. Knuth\n"
          ]
        }
      ],
      "source": [
        "enc = tiktoken.get_encoding('cl100k_base')\n",
        "sample_text = 'The art of Computer Programming by Donald E. Knuth'\n",
        "\n",
        "tokens = enc.encode(sample_text)\n",
        "print('Tokens:', tokens)\n",
        "print('Number of tokens:', len(tokens))\n",
        "\n",
        "decoded = enc.decode(tokens)\n",
        "print('Decoded back:', decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualização colorida com `rich`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────────────────────────────────── </span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\"> Token Visualization </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────────────────────────────────</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[92m────────────────────────────────────────────── \u001b[0m\u001b[1;34m Token Visualization \u001b[0m\u001b[92m ──────────────────────────────────────────────\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">The</span> <span style=\"color: #008000; text-decoration-color: #008000\"> art</span> <span style=\"color: #000080; text-decoration-color: #000080\"> of</span> <span style=\"color: #808000; text-decoration-color: #808000\"> Computer</span> <span style=\"color: #800080; text-decoration-color: #800080\"> Programming</span> <span style=\"color: #008080; text-decoration-color: #008080\"> by</span> <span style=\"color: #800000; text-decoration-color: #800000\"> Donald</span> <span style=\"color: #008000; text-decoration-color: #008000\"> E</span> <span style=\"color: #000080; text-decoration-color: #000080\">.</span> <span style=\"color: #808000; text-decoration-color: #808000\"> Kn</span> <span style=\"color: #800080; text-decoration-color: #800080\">uth</span> \n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31mThe\u001b[0m \u001b[32m art\u001b[0m \u001b[34m of\u001b[0m \u001b[33m Computer\u001b[0m \u001b[35m Programming\u001b[0m \u001b[36m by\u001b[0m \u001b[31m Donald\u001b[0m \u001b[32m E\u001b[0m \u001b[34m.\u001b[0m \u001b[33m Kn\u001b[0m \u001b[35muth\u001b[0m \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Number of tokens:</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mNumber of tokens:\u001b[0m \u001b[1;36m11\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from rich.console import Console\n",
        "from rich.text import Text\n",
        "\n",
        "console = Console()\n",
        "\n",
        "def visualize_tokens(text, encoding):\n",
        "    tokens = encoding.encode(text)\n",
        "    decoded_tokens = [encoding.decode([t]) for t in tokens]\n",
        "\n",
        "    rich_text = Text()\n",
        "    colors = [\"red\", \"green\", \"blue\", \"yellow\", \"magenta\", \"cyan\"]\n",
        "\n",
        "    for i, token_str in enumerate(decoded_tokens):\n",
        "        color = colors[i % len(colors)]\n",
        "        rich_text.append(token_str, style=color)\n",
        "        rich_text.append(\" \")  # espaço entre tokens\n",
        "\n",
        "    console.rule(\"[bold blue] Token Visualization [/bold blue]\")\n",
        "    console.print(rich_text)\n",
        "    console.print(f\"[bold]Number of tokens:[/bold] {len(tokens)}\")\n",
        "\n",
        "# Exemplo de uso\n",
        "visualize_tokens(sample_text, enc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code snippet demonstrates why “tokens are not equal to words”: common words may map to a single token, while less frequent or morphologically complex words are decomposed into several subword units."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palavra: art\n",
            "Tokens: [472]\n",
            "Divisão: ['art']\n",
            "\n",
            "Palavra: database\n",
            "Tokens: [12494]\n",
            "Divisão: ['database']\n",
            "\n",
            "Palavra: artificially\n",
            "Tokens: [472, 16895, 398]\n",
            "Divisão: ['art', 'ificial', 'ly']\n",
            "\n",
            "Palavra: tokenization\n",
            "Tokens: [5963, 2065]\n",
            "Divisão: ['token', 'ization']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Exemplo: visualizar divisão de palavras em múltiplos tokens\n",
        "examples = [\"art\", \"database\", \"artificially\", \"tokenization\"]\n",
        "\n",
        "for word in examples:\n",
        "    tokens = enc.encode(word)\n",
        "    parts = [enc.decode([t]) for t in tokens]\n",
        "    print(f\"Palavra: {word}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Divisão: {parts}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- **Tokens are not equal to words**. Sometimes a word = 1 token, sometimes it splits into several.\n",
        "- **Subword tokenization is statistical, not semantic.** The splits reflect patterns learned from frequency in the training corpus, not linguistic structure. For example, “artificially” is broken into parts that are efficient for the model, not necessarily meaningful for humans.\n",
        "- **Token counts can be counterintuitive.** A common word like “database” is a single token. A morphologically simpler but less frequent form like “artificially” may become three tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Estimate token usage\n",
        "You can use `tiktoken` to calculate how many tokens are in a prompt. This is useful to:\n",
        "- Avoid exceeding the **context window**.\n",
        "- Estimate API **costs** (OpenAI charges per token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Characters: 980\n",
            "Tokens: 121\n"
          ]
        }
      ],
      "source": [
        "long_text = 'Artificial intelligence will transform databases.' * 20\n",
        "print('Characters:', len(long_text))\n",
        "print('Tokens:', len(enc.encode(long_text)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercises\n",
        "1. Try different encodings: `gpt2`, `p50k_base`.\n",
        "2. Count tokens for a long paragraph (e.g., from your notes).\n",
        "3. Compare `len(text)` (characters) with number of tokens.\n",
        "4. What happens if you try to encode a very long text (close to 4000 tokens)?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sbbd2025_course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
