{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# From LLMs to Minimal Agents\n",
        "## Learning Goals\n",
        "- Understand the agent paradigm: LLM = brain, tools = body.\n",
        "- Build a minimal agent loop with two tools.\n",
        "- Experience **decision → action → result** cycle.\n",
        "\n",
        "This notebook connects with Section *1.3 From LLMs to LLM-based Agents* of the lecture notes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Define the tools\n",
        "Our agent can:\n",
        "1. Use a **calculator** to evaluate math expressions.\n",
        "2. Use a **dictionary** to define words.\n",
        "\n",
        "> The agent (through its brain, an LLM) will decide which tool to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculator_tool(expression: str) -> str:\n",
        "    try:\n",
        "        return str(eval(expression))\n",
        "    except:\n",
        "        return 'Error in calculation'\n",
        "\n",
        "dictionary = {\n",
        "    'agent': 'An autonomous problem-solving system.',\n",
        "    'database': 'An organized collection of structured information.'\n",
        "}\n",
        "def dictionary_tool(word: str) -> str:\n",
        "    return dictionary.get(word.lower(), 'Word not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Initialize an LLM\n",
        "We use HuggingFace via LangChain.  \n",
        "⚠️ Replace with ChatOpenAI if using OpenAI API, or ChatOllama for local models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load get_llm.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "# Load environment variables from .env\n",
        "load_dotenv()\n",
        "\n",
        "def get_llm(provider: str = \"openai\"):\n",
        "    \"\"\"\n",
        "    Return a language model instance configured for either OpenAI or Ollama.\n",
        "\n",
        "    This function centralizes the initialization of chat-based LLMs so that \n",
        "    notebooks and applications can switch seamlessly between cloud-based models \n",
        "    (OpenAI) and local models (Ollama).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    provider : str, optional\n",
        "        The backend provider to use. Options are:\n",
        "        - \"openai\": returns a ChatOpenAI instance (requires OPENAI_API_KEY in .env).\n",
        "        - \"ollama\": returns a ChatOllama instance (requires Ollama installed locally).\n",
        "        Default is \"openai\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    langchain.chat_models.base.BaseChatModel\n",
        "        A chat model instance that can be invoked with messages.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    Initialize an OpenAI model (requires API key):\n",
        "\n",
        "    >>> llm = get_llm(\"openai\")\n",
        "    >>> llm.invoke(\"Hello, how are you?\")\n",
        "\n",
        "    Initialize a local Ollama model (e.g., Gemma2 2B):\n",
        "\n",
        "    >>> llm = get_llm(\"ollama\")\n",
        "    >>> llm.invoke(\"Summarize the benefits of reinforcement learning.\")\n",
        "    \"\"\"\n",
        "    if provider == \"openai\":\n",
        "        return ChatOpenAI(\n",
        "            model=\"gpt-4o-mini\",  # can also be \"gpt-4.1\" or \"gpt-4o\"\n",
        "            temperature=0\n",
        "        )\n",
        "    elif provider == \"ollama\":\n",
        "        return ChatOllama(\n",
        "            model=\"gemma2:2b\",   # replace with any local model installed in Ollama\n",
        "            temperature=0\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported provider. Use 'openai' or 'ollama'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Build the agent loop\n",
        "- The **system prompt** explains available tools.\n",
        "- The **LLM decides** which tool to use.\n",
        "- The **agent executes** the tool and returns the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "def minimal_agent(query: str):\n",
        "    llm = get_llm()\n",
        "    \n",
        "    system_prompt = \"\"\"\n",
        "    You are a minimal AI agent.\n",
        "    Available tools:\n",
        "    1. Calculator: for math expressions.\n",
        "    2. Dictionary: for word definitions.\n",
        "\n",
        "    Always respond with a valid JSON object in the format:\n",
        "    {\"tool\": \"calculator\", \"input\": \"2+2\"}\n",
        "    or\n",
        "    {\"tool\": \"dictionary\", \"input\": \"agent\"}\n",
        "    \"\"\"\n",
        "    \n",
        "    decision = llm.invoke([\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=query)\n",
        "    ])\n",
        "\n",
        "    print(\"LLM decision:\", decision.content)\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(decision.content)\n",
        "        tool = parsed.get(\"tool\", \"\").lower()\n",
        "        argument = parsed.get(\"input\", \"\")\n",
        "    except Exception as e:\n",
        "        return f\"Could not parse decision: {e}\"\n",
        "\n",
        "    if tool == \"calculator\":\n",
        "        return calculator_tool(argument)\n",
        "    elif tool == \"dictionary\":\n",
        "        return dictionary_tool(argument)\n",
        "    else:\n",
        "        return \"No valid tool selected.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Run examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM decision: {\"tool\": \"calculator\", \"input\": \"15*3\"}\n",
            "45\n",
            "LLM decision: {\"tool\": \"dictionary\", \"input\": \"agent\"}\n",
            "An autonomous problem-solving system.\n",
            "LLM decision: {\"tool\": \"dictionary\", \"input\": \"database\"}\n",
            "An organized collection of structured information.\n"
          ]
        }
      ],
      "source": [
        "print(minimal_agent('What is 15 * 3?'))\n",
        "print(minimal_agent('Define: agent'))\n",
        "print(minimal_agent('Define: database'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- The LLM functions as the agent’s brain, reasoning about the query and producing a structured JSON decision.\n",
        "\n",
        "- The agent functions as the body, parsing the JSON and executing the selected tool with the provided input.\n",
        "\n",
        "- This separation of concerns illustrates the foundation of tool-calling mechanisms later formalized in patterns such as ReAct and implemented in frameworks like LangGraph.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Add a new tool: a translator that translates English to Portuguese, and extend the system prompt to include it.\n",
        "\n",
        "2. Modify the agent to loop until it returns a final JSON object with the key \"answer\" instead of only one decision.\n",
        "\n",
        "3. Replace ChatOpenAI with ChatOllama in the get_llm() function to run the agent with a local model (e.g., gemma2:2b)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sbbd2025_course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
