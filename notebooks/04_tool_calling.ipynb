{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tool Calling with LangChain (Runnable API)\n",
        "\n",
        "## Learning Goals\n",
        "- Define callable tools with clear input schemas.\n",
        "- Drive tool selection from an LLM using **structured JSON outputs**.\n",
        "- Compose a minimal **Runnable** pipeline (`|` and `.invoke`) that parses the LLM decision and dispatches the selected tool.\n",
        "- Understand how this pattern relates to ReAct and paves the way for LangGraph.\n",
        "\n",
        "This notebook corresponds to the *Tool Calling* sub-section and uses the modern **Runnable API** in LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load get_llm.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "# Load environment variables from .env\n",
        "load_dotenv()\n",
        "\n",
        "def get_llm(provider: str = \"openai\"):\n",
        "    \"\"\"\n",
        "    Return a language model instance configured for either OpenAI or Ollama.\n",
        "\n",
        "    This function centralizes the initialization of chat-based LLMs so that \n",
        "    notebooks and applications can switch seamlessly between cloud-based models \n",
        "    (OpenAI) and local models (Ollama).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    provider : str, optional\n",
        "        The backend provider to use. Options are:\n",
        "        - \"openai\": returns a ChatOpenAI instance (requires OPENAI_API_KEY in .env).\n",
        "        - \"ollama\": returns a ChatOllama instance (requires Ollama installed locally).\n",
        "        Default is \"openai\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    langchain.chat_models.base.BaseChatModel\n",
        "        A chat model instance that can be invoked with messages.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    Initialize an OpenAI model (requires API key):\n",
        "\n",
        "    >>> llm = get_llm(\"openai\")\n",
        "    >>> llm.invoke(\"Hello, how are you?\")\n",
        "\n",
        "    Initialize a local Ollama model (e.g., Gemma2 2B):\n",
        "\n",
        "    >>> llm = get_llm(\"ollama\")\n",
        "    >>> llm.invoke(\"Summarize the benefits of reinforcement learning.\")\n",
        "    \"\"\"\n",
        "    if provider == \"openai\":\n",
        "        return ChatOpenAI(\n",
        "            model=\"gpt-4o-mini\",  # can also be \"gpt-4.1\" or \"gpt-4o\"\n",
        "            temperature=0\n",
        "        )\n",
        "    elif provider == \"ollama\":\n",
        "        return ChatOllama(\n",
        "            model=\"gemma2:2b\",   # replace with any local model installed in Ollama\n",
        "            temperature=0\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported provider. Use 'openai' or 'ollama'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import json\n",
        "from typing import Literal\n",
        "from pydantic import BaseModel, Field\n",
        "from dotenv import load_dotenv\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# It is assumed that get_llm() is defined elsewhere (Notebook 2 helper)\n",
        "llm = get_llm(\"openai\")  # or get_llm(\"ollama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1 - Define tools with clear input schemas\n",
        "Two tools are defined: a **calculator** and a **dictionary**. Each tool has a minimal validation layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Evaluate a simple Python arithmetic expression safely.\"\"\"\n",
        "    try:\n",
        "        # Very constrained eval; in production use a proper expression parser\n",
        "        return str(eval(expression, {\"__builtins__\": {}}, {}))\n",
        "    except Exception as e:\n",
        "        return f\"Calculator error: {e}\"\n",
        "\n",
        "DICTIONARY = {\n",
        "    \"agent\": \"An autonomous problem-solving system.\",\n",
        "    \"database\": \"An organized collection of structured information.\",\n",
        "    \"token\": \"A basic unit of text used by language models.\",\n",
        "}\n",
        "\n",
        "def define_word(word: str) -> str:\n",
        "    return DICTIONARY.get(word.lower(), \"Word not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2 - Specify the decision schema\n",
        "A Pydantic model is used so the LLM is guided to return structured JSON that can be parsed reliably."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToolDecision(BaseModel):\n",
        "    tool: Literal[\"calculator\", \"dictionary\"] = Field(..., description=\"Tool to call\")\n",
        "    input: str = Field(..., description=\"Argument to pass to the selected tool\")\n",
        "\n",
        "def parse_json(text: str) -> ToolDecision:\n",
        "    try:\n",
        "        data = json.loads(text)\n",
        "        return ToolDecision(**data)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Invalid tool decision JSON: {e}\\nRaw: {text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3 - Prompt the LLM to produce a valid JSON decision\n",
        "A system prompt describes the tools and the required output format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_INSTRUCTIONS = (\n",
        "    \"You are a tool-choosing assistant. \"\n",
        "    \"Given a user query, choose exactly one tool and provide a valid JSON object.\\n\"\n",
        "    \"Available tools:\\n\"\n",
        "    \"- calculator: evaluate arithmetic expressions. Example: {{\\\"tool\\\": \\\"calculator\\\", \\\"input\\\": \\\"12*7\\\"}}\\n\"\n",
        "    \"- dictionary: provide short definitions. Example: {{\\\"tool\\\": \\\"dictionary\\\", \\\"input\\\": \\\"agent\\\"}}\\n\\n\"\n",
        "    \"Respond with JSON only. No extra text.\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", SYSTEM_INSTRUCTIONS),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "pipeline = (\n",
        "    prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | RunnableLambda(parse_json)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4 - Dispatch the selected tool\n",
        "A dispatcher maps `tool` â†’ function. The entire flow is kept inside the Runnable chain for clarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "TOOLBOX = {\n",
        "    \"calculator\": calculator,\n",
        "    \"dictionary\": define_word,\n",
        "}\n",
        "\n",
        "def dispatch(decision: ToolDecision) -> str:\n",
        "    tool_fn = TOOLBOX.get(decision.tool)\n",
        "    if tool_fn is None:\n",
        "        return f\"Unknown tool: {decision.tool}\"\n",
        "    return tool_fn(decision.input)\n",
        "\n",
        "tool_runner = RunnableLambda(dispatch)\n",
        "\n",
        "agent = pipeline | tool_runner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5 - Run examples\n",
        "Two queries are tested: a math question and a definition request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45\n",
            "An autonomous problem-solving system.\n"
          ]
        }
      ],
      "source": [
        "print(agent.invoke({\"question\": \"What is 15 * 3?\"}))\n",
        "print(agent.invoke({\"question\": \"Define: agent\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Difference between notebooks 2 and 4\n",
        "\n",
        "## Notebook 2 - Minimal Agent\n",
        "\n",
        "- Purpose: introduce the idea of an agent.\n",
        "\n",
        "- Implementation style:\n",
        "\n",
        "  - Direct calls to the LLM (llm.invoke) with a handcrafted system prompt.\n",
        "  - Very simple string parsing (later improved to JSON parsing).\n",
        "  - Tools (calculator, dictionary) are plain Python functions.\n",
        "\n",
        "- Key learning point: the separation between the LLM as the brain (decides) and the agent as the body (executes).\n",
        "\n",
        "- Analogy: like wiring components manually to show the principle.\n",
        "\n",
        "## Notebook 4 (this one) - Tool Calling\n",
        "\n",
        "- Purpose: show how LangChainâ€™s modern Runnable API structures tool calling.\n",
        "\n",
        "- Implementation style:\n",
        "\n",
        "    - Uses ChatPromptTemplate + Runnable composition (| operator, .invoke).\n",
        "\n",
        "    - Defines a Pydantic schema (ToolDecision) for structured outputs.\n",
        "\n",
        "    - Explicit dispatcher mapping tool names â†’ Python functions.\n",
        "\n",
        "    - Enforces valid JSON and integrates error handling.\n",
        "\n",
        "- Key learning point: this is a robust, extensible pattern, closer to what production LangChain agents do (and a stepping stone toward LangGraph).\n",
        "\n",
        "- Analogy: like replacing a hand-wired circuit with a clean modular board."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- The LLM acts as a **router**, returning a JSON decision that identifies the tool and the input.\n",
        "- The Runnable chain enforces **deterministic parsing** and **safe dispatch**.\n",
        "- This is a minimal tool-calling agent; the same composition is expandable with additional tools and validation.\n",
        "- The pattern connects naturally to **ReAct** (reason â†’ action) and to **LangGraph** for multi-step control flows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "1. Add a **translator** tool (English â†’ Portuguese) and integrate it into `TOOLBOX` and the system instructions.\n",
        "2. Extend the JSON schema to include an optional `explanation` field that the LLM can fill with a short rationale; log it but ignore it for execution.\n",
        "3. Replace `get_llm(\\\"openai\\\")` with `get_llm(\\\"ollama\\\")` and test with a local model (e.g., `gemma2:2b`)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (sbbd2025_course)",
      "language": "python",
      "name": "sbbd2025_course"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
