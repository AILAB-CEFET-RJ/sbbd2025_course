{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompting Techniques and Interaction Patterns\n",
        "\n",
        "## Learning Goals\n",
        "- Explore basic prompting techniques: **zero-shot**, **few-shot**, and **chain-of-thought (CoT)**.\n",
        "- Understand how prompting guides reasoning depth and style.\n",
        "- Implement **interaction patterns**: ReAct and Plan-and-Act.\n",
        "- Connect these methods to the design of LLM-based agents.\n",
        "\n",
        "This notebook corresponds to Section *1.4 From Prompting Techniques to Interaction Patterns* in the lecture notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load get_llm.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "# Load environment variables from .env\n",
        "load_dotenv()\n",
        "\n",
        "def get_llm(provider: str = \"openai\"):\n",
        "    \"\"\"\n",
        "    Return a language model instance configured for either OpenAI or Ollama.\n",
        "\n",
        "    This function centralizes the initialization of chat-based LLMs so that \n",
        "    notebooks and applications can switch seamlessly between cloud-based models \n",
        "    (OpenAI) and local models (Ollama).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    provider : str, optional\n",
        "        The backend provider to use. Options are:\n",
        "        - \"openai\": returns a ChatOpenAI instance (requires OPENAI_API_KEY in .env).\n",
        "        - \"ollama\": returns a ChatOllama instance (requires Ollama installed locally).\n",
        "        Default is \"openai\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    langchain.chat_models.base.BaseChatModel\n",
        "        A chat model instance that can be invoked with messages.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    Initialize an OpenAI model (requires API key):\n",
        "\n",
        "    >>> llm = get_llm(\"openai\")\n",
        "    >>> llm.invoke(\"Hello, how are you?\")\n",
        "\n",
        "    Initialize a local Ollama model (e.g., Gemma2 2B):\n",
        "\n",
        "    >>> llm = get_llm(\"ollama\")\n",
        "    >>> llm.invoke(\"Summarize the benefits of reinforcement learning.\")\n",
        "    \"\"\"\n",
        "    if provider == \"openai\":\n",
        "        return ChatOpenAI(\n",
        "            model=\"gpt-4o-mini\",  # can also be \"gpt-4.1\" or \"gpt-4o\"\n",
        "            temperature=0\n",
        "        )\n",
        "    elif provider == \"ollama\":\n",
        "        return ChatOllama(\n",
        "            model=\"gemma2:2b\",   # replace with any local model installed in Ollama\n",
        "            temperature=0\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported provider. Use 'openai' or 'ollama'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "llm = get_llm(\"openai\")  # or get_llm(\"ollama\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zero-shot prompting\n",
        "The model is asked to perform a task **without examples**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The capital of Brazil is Brasília.\n"
          ]
        }
      ],
      "source": [
        "question = \"What is the capital of Brazil?\"\n",
        "response = llm.invoke([HumanMessage(content=question)])\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Few-shot prompting\n",
        "The model is given **examples** to guide its output format and style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Brasília\n"
          ]
        }
      ],
      "source": [
        "examples = \"\"\"\n",
        "Q: What is the capital of France?\n",
        "A: Paris\n",
        "\n",
        "Q: What is the capital of Japan?\n",
        "A: Tokyo\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"{examples}\n",
        "Q: What is the capital of Brazil?\n",
        "A:\"\"\"\n",
        "\n",
        "response = llm.invoke([HumanMessage(content=prompt)])\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chain-of-Thought (CoT)\n",
        "The model is instructed to **reason step by step** before answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To find the product of 15 and 3, we can break it down step by step:\n",
            "\n",
            "1. **Understand multiplication**: Multiplication is essentially repeated addition. So, multiplying 15 by 3 means we are adding 15 three times.\n",
            "\n",
            "2. **Set up the addition**: \n",
            "   - 15 + 15 + 15\n",
            "\n",
            "3. **Calculate the first two 15s**:\n",
            "   - 15 + 15 = 30\n",
            "\n",
            "4. **Add the third 15**:\n",
            "   - 30 + 15 = 45\n",
            "\n",
            "So, 15 multiplied by 3 equals 45. \n",
            "\n",
            "Therefore, \\( 15 \\times 3 = 45 \\).\n"
          ]
        }
      ],
      "source": [
        "cot_prompt = \"What is 15 multiplied by 3? Let's think step by step.\"\n",
        "response = llm.invoke([HumanMessage(content=cot_prompt)])\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ReAct: Reason + Act\n",
        "The model alternates between **thinking** and **acting** by calling tools.\n",
        "\n",
        "We simulate this with two simple tools:\n",
        "- Calculator\n",
        "- Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"tool\": \"calculator\", \"input\": \"12*7\"}\n"
          ]
        }
      ],
      "source": [
        "tools_description = \"\"\"\n",
        "You can use these tools:\n",
        "- Calculator: perform math (e.g., {\"tool\": \"calculator\", \"input\": \"2+2\"})\n",
        "- Dictionary: define words (e.g., {\"tool\": \"dictionary\", \"input\": \"agent\"})\n",
        "Always output valid JSON when using a tool.\n",
        "\"\"\"\n",
        "\n",
        "react_prompt = f\"\"\"{tools_description}\n",
        "\n",
        "Question: What is 12 * 7?\n",
        "\"\"\"\n",
        "\n",
        "response = llm.invoke([\n",
        "    SystemMessage(content=\"You are an AI agent that reasons step by step.\"),\n",
        "    HumanMessage(content=react_prompt)\n",
        "])\n",
        "\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plan-and-Act\n",
        "This pattern separates **planning** from **execution**.\n",
        "- First, the model produces a plan in natural language.\n",
        "- Then, the agent executes each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plan:\n",
            " ### Steps to Find the Square Root of 144\n",
            "\n",
            "1. **Understand the Concept**: Recognize that the square root of a number is a value that, when multiplied by itself, gives the original number.\n",
            "\n",
            "2. **Identify the Number**: In this case, the number is 144.\n",
            "\n",
            "3. **Use a Calculator (if needed)**: If you have access to a calculator, you can simply input 144 and use the square root function.\n",
            "\n",
            "4. **Estimate the Square Root**: If you don't have a calculator, think of perfect squares close to 144. For example, 12 x 12 = 144.\n",
            "\n",
            "5. **Confirm the Result**: Verify that 12 multiplied by itself equals 144.\n",
            "\n",
            "6. **State the Answer**: Conclude that the square root of 144 is 12.\n",
            "\n",
            "### Steps to Define the Word 'Agent'\n",
            "\n",
            "1. **Identify the Context**: Determine the context in which the word 'agent' is being used (e.g., business, science, law, etc.).\n",
            "\n",
            "2. **Research the Definition**: Look up the definition in a reliable dictionary or resource. \n",
            "\n",
            "3. **Consider Different Meanings**: Note that 'agent' can have multiple meanings depending on the context:\n",
            "   - In business, it refers to a person who acts on behalf of another.\n",
            "   - In science, it can refer to a substance that produces an effect (e.g., a chemical agent).\n",
            "   - In law, it refers to someone authorized to act for another.\n",
            "\n",
            "4. **Summarize the Definition**: Write a concise definition that encompasses the primary meanings of the word.\n",
            "\n",
            "5. **Provide Examples**: If necessary, include examples to illustrate the different uses of the word 'agent'.\n",
            "\n",
            "6. **Review and Finalize**: Ensure the definition is clear and accurate before finalizing it. \n",
            "\n",
            "By following these steps, you can effectively find the square root of 144 and define the word 'agent'.\n"
          ]
        }
      ],
      "source": [
        "plan_prompt = \"\"\"\n",
        "You are a planning assistant.\n",
        "Question: What steps are needed to find the square root of 144 and define the word 'agent'?\n",
        "Respond with a step-by-step plan in numbered steps.\n",
        "\"\"\"\n",
        "\n",
        "plan = llm.invoke([HumanMessage(content=plan_prompt)])\n",
        "print(\"Plan:\\n\", plan.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection\n",
        "- Zero-shot and few-shot prompting demonstrate the importance of examples.\n",
        "- Chain-of-thought prompting elicits **explicit reasoning**.\n",
        "- ReAct and Plan-and-Act are **interaction patterns** that structure reasoning with actions.\n",
        "- These patterns anticipate how modern frameworks like LangGraph organize multi-step workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "1. Modify the few-shot example to include **wrong examples**. What happens?\n",
        "2. Extend the ReAct loop to include an additional tool, such as a **translator**.\n",
        "3. In the Plan-and-Act section, implement execution of each step (calculator + dictionary)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sbbd2025_course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
